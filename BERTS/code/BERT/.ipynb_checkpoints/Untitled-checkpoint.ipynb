{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59519a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install bitsandbytes\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d20954",
   "metadata": {},
   "source": [
    "## Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c5741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/lib/python3.8/dist-packages/torch/lib:/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-4f228fc8-df9d-40e7-9a20-5e6a46d772d9.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#General imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gdown \n",
    "import os\n",
    "import requests\n",
    "import string\n",
    "import re \n",
    "import shutil\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#Related to transformers and models\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "#Result imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#GENERAL CONSTANTS\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a395184",
   "metadata": {},
   "source": [
    "## Descarga del modelo a fine tunear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a37be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bhadresh-savani/bert-base-go-emotion\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "modelo_base  = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-go-emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463d724",
   "metadata": {},
   "source": [
    "## Descarga y tratamiento de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58204f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "      <td>on days when i feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "      <td>every time i imagine that someone i love or i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "      <td>when i had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "      <td>when i think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "      <td>at a gathering i found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>shame</td>\n",
       "      <td>Two years back someone invited me to be the tu...</td>\n",
       "      <td>two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>shame</td>\n",
       "      <td>I had taken the responsibility to do something...</td>\n",
       "      <td>i had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>fear</td>\n",
       "      <td>I was at home and I heard a loud sound of spit...</td>\n",
       "      <td>i was at home and i heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7514</th>\n",
       "      <td>guilt</td>\n",
       "      <td>I did not do the homework that the teacher had...</td>\n",
       "      <td>i did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>fear</td>\n",
       "      <td>I had shouted at my younger brother and he was...</td>\n",
       "      <td>i had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7516 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text  \\\n",
       "0         joy  On days when I feel close to my partner and ot...   \n",
       "1        fear  Every time I imagine that someone I love or I ...   \n",
       "2       anger  When I had been obviously unjustly treated and...   \n",
       "3     sadness  When I think about the short time that we live...   \n",
       "4     disgust  At a gathering I found myself involuntarily si...   \n",
       "...       ...                                                ...   \n",
       "7511    shame  Two years back someone invited me to be the tu...   \n",
       "7512    shame  I had taken the responsibility to do something...   \n",
       "7513     fear  I was at home and I heard a loud sound of spit...   \n",
       "7514    guilt  I did not do the homework that the teacher had...   \n",
       "7515     fear  I had shouted at my younger brother and he was...   \n",
       "\n",
       "                                         Text_processed  \n",
       "0     on days when i feel close to my partner and ot...  \n",
       "1     every time i imagine that someone i love or i ...  \n",
       "2     when i had been obviously unjustly treated and...  \n",
       "3     when i think about the short time that we live...  \n",
       "4     at a gathering i found myself involuntarily si...  \n",
       "...                                                 ...  \n",
       "7511  two years back someone invited me to be the tu...  \n",
       "7512  i had taken the responsibility to do something...  \n",
       "7513  i was at home and i heard a loud sound of spit...  \n",
       "7514  i did not do the homework that the teacher had...  \n",
       "7515  i had shouted at my younger brother and he was...  \n",
       "\n",
       "[7516 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fichero se encuentra en enlace externo\n",
    "url = \"https://raw.githubusercontent.com/PoorvaRane/Emotion-Detector/master/ISEAR.csv\"\n",
    "output_file = \"ISEAR.csv\"\n",
    "\n",
    "destination_folder = \"data\"\n",
    "#Si no existe la carpeta en la que queremos guardar los datos debemos crearla\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(output_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "shutil.move(output_file, f\"{destination_folder}/{output_file}\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = load_and_preprocess_data('./data/ISEAR.csv')\n",
    "df['Emotion'] = df['Emotion'].replace('guit', 'guilt')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171df0",
   "metadata": {},
   "source": [
    "## Probamos el modelo a ver de cuantas clases dispone  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e15b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'admiration', 'score': 0.0022283862344920635}, {'label': 'amusement', 'score': 0.00207656342536211}, {'label': 'anger', 'score': 0.6765081286430359}, {'label': 'annoyance', 'score': 0.11713651567697525}, {'label': 'approval', 'score': 0.006938391365110874}, {'label': 'caring', 'score': 0.008048061281442642}, {'label': 'confusion', 'score': 0.0015102234901860356}, {'label': 'curiosity', 'score': 0.0022445875220000744}, {'label': 'desire', 'score': 0.0015093624824658036}, {'label': 'disappointment', 'score': 0.018241364508867264}, {'label': 'disapproval', 'score': 0.016932494938373566}, {'label': 'disgust', 'score': 0.04833042994141579}, {'label': 'embarrassment', 'score': 0.0041035753674805164}, {'label': 'excitement', 'score': 0.002686750376597047}, {'label': 'fear', 'score': 0.0032824971713125706}, {'label': 'gratitude', 'score': 0.0020939591340720654}, {'label': 'grief', 'score': 0.002857409417629242}, {'label': 'joy', 'score': 0.0016693678917363286}, {'label': 'love', 'score': 0.0010952280135825276}, {'label': 'nervousness', 'score': 0.002996481955051422}, {'label': 'optimism', 'score': 0.0016878662863746285}, {'label': 'pride', 'score': 0.0007378275040537119}, {'label': 'realization', 'score': 0.003398278495296836}, {'label': 'relief', 'score': 0.0007681022398173809}, {'label': 'remorse', 'score': 0.0033701001666486263}, {'label': 'sadness', 'score': 0.016302889212965965}, {'label': 'surprise', 'score': 0.003571051638573408}, {'label': 'neutral', 'score': 0.04767394810914993}]]\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = TextClassificationPipeline(model=modelo_base, tokenizer=tokenizer, return_all_scores=True)\n",
    "resultados = pipe(\"fuck you leave me alone\")\n",
    "print(resultados)\n",
    "accum = 0\n",
    "for elemento in resultados: \n",
    "    for cuenta in elemento:\n",
    "        accum = accum + 1 \n",
    "print(accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46ed0d",
   "metadata": {},
   "source": [
    "Como observamos en este modelo se tienen 28 clases (clásico del dataset de goemotion), ahora tenemos varias opciones, podemos intentar modificar esta ultima capa para poder trabajar con los logits, lo cual numéricamente es más estable o añadir una capa final de clasificación después de esta para clasificar entre las emociones de ekman que estamos utilizando en nuestro estudio. \n",
    "\n",
    "No sabemos el método que han usado para clasificar las emociones. Por tanto lo primero que vamos a hacer va a ser realizar un entrenamiento básico añadiendo una capa final para clasificar lo que nos interesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4505b",
   "metadata": {},
   "source": [
    "## Exploración del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bhadresh-savani/bert-base-go-emotion\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMultilabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(modelo_base.config)\n",
    "print(modelo_base.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f0456",
   "metadata": {},
   "source": [
    "El objeto `BertConfig` contiene los parámetros y configuraciones necesarios para el modelo BERT utilizado en la clasificación de secuencias con múltiples etiquetas. A continuación se presenta una explicación de los campos clave en la configuración:\n",
    "\n",
    "- `hidden_size` y `num_attention_heads`: Estos parámetros determinan la dimensionalidad de los vectores ocultos y el número de cabezas de atención en el modelo BERT, respectivamente.\n",
    "\n",
    "- `num_hidden_layers`: Indica el número de capas ocultas en el modelo. Aumentar este valor puede aumentar la capacidad del modelo para aprender representaciones más complejas.\n",
    "\n",
    "- `dropout`: Controla la tasa de abandono (dropout) aplicada a las salidas de las capas ocultas para evitar el sobreajuste y mejorar la generalización.\n",
    "\n",
    "- `initializer_range`: Especifica el rango de inicialización para los pesos del modelo.\n",
    "\n",
    "- `max_position_embeddings`: Define la longitud máxima permitida de las secuencias de entrada.\n",
    "\n",
    "- `vocab_size`: Indica el tamaño del vocabulario utilizado por el modelo.\n",
    "\n",
    "Estos campos son cruciales para configurar correctamente el modelo BERT en la clasificación de secuencias con múltiples etiquetas. Sin embargo, también debemos considerar otros aspectos como el modelo preentrenado, el conjunto de datos y los hiperparámetros específicos del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc9d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2273, -4.7028, -5.9401, -5.2675, -3.2153, -4.5923, -5.4052, -4.9632,\n",
      "         -4.6493, -5.4046, -5.5101, -6.3988, -6.0861, -3.6266, -6.5459, -4.0359,\n",
      "         -7.2671, -2.7496,  1.5563, -7.0032, -4.6198, -6.0098, -4.3959, -6.7773,\n",
      "         -6.5881, -5.3570, -5.3487, -3.5594]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "texto = \"I love this movie!\"\n",
    "inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = modelo_base(**inputs)\n",
    "\n",
    "# Obtener los logits\n",
    "logits = outputs.logits\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f7d4d",
   "metadata": {},
   "source": [
    "Ahora queremos obtener cual es la clase que es más probable que se haya predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b118d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha (índice): 18\n",
      "Clase predicha (texto): love\n"
     ]
    }
   ],
   "source": [
    "probabilidades = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Obtener la clase con la probabilidad más alta\n",
    "clase_predicha_idx = np.argmax(probabilidades.detach().numpy())\n",
    "clase_predicha = modelo_base.config.id2label[clase_predicha_idx]\n",
    "\n",
    "print(\"Clase predicha (índice):\", clase_predicha_idx)\n",
    "print(\"Clase predicha (texto):\", clase_predicha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c1ca8",
   "metadata": {},
   "source": [
    "## Creación de la clase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef29e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiModelo(nn.Module):\n",
    "    def __init__(self, modelo_base, num_clases):\n",
    "        super(MiModelo, self).__init__()\n",
    "        \n",
    "        # Definimos el modelo base que estará preentrenado\n",
    "        self.modelo_base = modelo_base\n",
    "\n",
    "        # Capa de dropout para evitar sobreajuste, con probabilidad de dropout de 0.5\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Capa totalmente conectada que transforma la salida del modelo base en una salida de tamaño 14\n",
    "        self.fc1 = nn.Linear(28, 14)\n",
    "        \n",
    "        # Función de activación ReLU\n",
    "        self.activacion = nn.ReLU()\n",
    "        \n",
    "        # Segunda capa totalmente conectada que transforma la salida de tamaño 14 a la cantidad de clases\n",
    "        self.fc2 = nn.Linear(14, num_clases)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pasamos los inputs a través del modelo base\n",
    "        outputs = self.modelo_base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Tomamos la salida del modelo base\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        # Aplicamos dropout a la salida del modelo base\n",
    "        pooled_output = self.dropout(last_hidden_state)\n",
    "\n",
    "        # Pasamos la salida por la primera capa completamente conectada\n",
    "        hidden = self.fc1(pooled_output)\n",
    "\n",
    "        # Aplicamos la función de activación\n",
    "        hidden = self.activacion(hidden)\n",
    "\n",
    "        # Pasamos la salida por la segunda capa completamente conectada para obtener los logits finales\n",
    "        logits = self.fc2(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, inputs, targets):\n",
    "        # Obtenemos los logits pasando los inputs por el modelo\n",
    "        logits = self.forward(inputs.input_ids, inputs.attention_mask)\n",
    "        \n",
    "        # Definimos la función de pérdida (Cross Entropy Loss)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Calculamos la pérdida\n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d941c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = 7\n",
    "mi_modelo = MiModelo(modelo_base=modelo_base, num_clases=num_clases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3d7ac",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe78600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df['Text_processed'].tolist()\n",
    "labels = df['Emotion'].tolist()\n",
    "\n",
    "# Convertir las etiquetas a números\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# tokenizar inputs\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    inputs,\n",
    "    padding='longest', \n",
    "    truncation=True, \n",
    "    max_length=512, \n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "967f61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear conjunto de datos\n",
    "dataset = MyDataset(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4829c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d71399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89b4b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = len(le.classes_)\n",
    "model = MiModelo(modelo_base, num_clases)\n",
    "\n",
    "total_layers = len(list(model.modelo_base.parameters()))\n",
    "unfrozen_layers = total_layers - 2  # Descongelar las últimas dos capas\n",
    "\n",
    "for i, param in enumerate(model.modelo_base.parameters()):\n",
    "    if i >= unfrozen_layers:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2b1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a660ec3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1088397419.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f'epoch: {epoch} train_loss {train_loss} val_loss {val_loss} val_accuracy {val_accuracy})\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "        val_accuracy = correct_predictions.double() / len(val_dataset)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "    print(f'epoch: {epoch} train_loss {train_loss} val_loss {val_loss} val_accuracy {val_accuracy}')\n",
    "\n",
    "    # Implement Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([acc.cpu() for acc in val_accuracies], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\", inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30335c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MiModelo.push_to_hub(repo_id=\"RikoteMaster/Bert_peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3191037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(mi_modelo, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d5924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
