{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.8/dist-packages (4.31.0.dev0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers[torch]) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.9->transformers[torch]) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.26.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.8/dist-packages (0.4.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from peft) (2.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from peft) (4.31.0.dev0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.8/dist-packages (from peft) (0.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (4.6.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (68.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.40.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.26.4)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers->peft) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->peft) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->peft) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers->peft) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.8/dist-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement hugingface_hub (from versions: none)\n",
      "ERROR: No matching distribution found for hugingface_hub\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#modifica estos installs y hazlos con la biblioteca  os\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "os.system('pip install datasets')\n",
    "os.system('pip install transformers[torch]')\n",
    "os.system('pip install accelerate>=0.20.1')\n",
    "os.system('pip install peft')\n",
    "os.system('pip install hugingface_hub')\n",
    "os.system('huggingface-cli login --token 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')\n",
    "\n",
    "\n",
    "# Semillas\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04377eee4454b648841fc56db2c4fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8734f7f6c7442200.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-8340c37bb5fe7f72.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f09616e24b2a666a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f3df3ba700b4e5bf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-49b8b126f15a8e7c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/RikoteMaster___parquet/RikoteMaster--isear_augmented-f3da66f3f32e5396/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-04c4f3619392abe0.arrow\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5100' max='32830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5100/32830 32:48 < 2:58:30, 2.59 it/s, Epoch 10/70]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.957258</td>\n",
       "      <td>0.142749</td>\n",
       "      <td>0.084093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.948871</td>\n",
       "      <td>0.166163</td>\n",
       "      <td>0.128887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.944134</td>\n",
       "      <td>0.172205</td>\n",
       "      <td>0.145954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.941703</td>\n",
       "      <td>0.181269</td>\n",
       "      <td>0.156599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.937710</td>\n",
       "      <td>0.219033</td>\n",
       "      <td>0.163046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.934661</td>\n",
       "      <td>0.228852</td>\n",
       "      <td>0.196381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.932850</td>\n",
       "      <td>0.206949</td>\n",
       "      <td>0.148814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.930289</td>\n",
       "      <td>0.228852</td>\n",
       "      <td>0.190490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.927231</td>\n",
       "      <td>0.254532</td>\n",
       "      <td>0.207237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.922867</td>\n",
       "      <td>0.245468</td>\n",
       "      <td>0.191531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.920786</td>\n",
       "      <td>0.258308</td>\n",
       "      <td>0.193527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.911594</td>\n",
       "      <td>0.253021</td>\n",
       "      <td>0.199360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.897946</td>\n",
       "      <td>0.276435</td>\n",
       "      <td>0.196329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.875713</td>\n",
       "      <td>0.286254</td>\n",
       "      <td>0.207895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.840723</td>\n",
       "      <td>0.314955</td>\n",
       "      <td>0.241398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.781329</td>\n",
       "      <td>0.320997</td>\n",
       "      <td>0.252529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.720186</td>\n",
       "      <td>0.338369</td>\n",
       "      <td>0.264908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.639700</td>\n",
       "      <td>0.392749</td>\n",
       "      <td>0.339538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.566340</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>0.359680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.517669</td>\n",
       "      <td>0.412387</td>\n",
       "      <td>0.350402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.492850</td>\n",
       "      <td>0.425227</td>\n",
       "      <td>0.369229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.455754</td>\n",
       "      <td>0.434290</td>\n",
       "      <td>0.384636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.435714</td>\n",
       "      <td>0.444864</td>\n",
       "      <td>0.404929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.408715</td>\n",
       "      <td>0.472054</td>\n",
       "      <td>0.440762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.386861</td>\n",
       "      <td>0.478097</td>\n",
       "      <td>0.445178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.372681</td>\n",
       "      <td>0.486405</td>\n",
       "      <td>0.450643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.345385</td>\n",
       "      <td>0.505287</td>\n",
       "      <td>0.466854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.838700</td>\n",
       "      <td>1.329142</td>\n",
       "      <td>0.505287</td>\n",
       "      <td>0.468827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.319034</td>\n",
       "      <td>0.511329</td>\n",
       "      <td>0.468405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.309513</td>\n",
       "      <td>0.519637</td>\n",
       "      <td>0.475427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.280747</td>\n",
       "      <td>0.537764</td>\n",
       "      <td>0.494435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.257637</td>\n",
       "      <td>0.543051</td>\n",
       "      <td>0.504251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.241611</td>\n",
       "      <td>0.548338</td>\n",
       "      <td>0.516470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.226845</td>\n",
       "      <td>0.543051</td>\n",
       "      <td>0.523197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.211611</td>\n",
       "      <td>0.553625</td>\n",
       "      <td>0.519760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.192473</td>\n",
       "      <td>0.561934</td>\n",
       "      <td>0.534683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.169408</td>\n",
       "      <td>0.574773</td>\n",
       "      <td>0.555829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.160503</td>\n",
       "      <td>0.574018</td>\n",
       "      <td>0.542743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.148949</td>\n",
       "      <td>0.577039</td>\n",
       "      <td>0.543606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.131012</td>\n",
       "      <td>0.584592</td>\n",
       "      <td>0.559915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.129901</td>\n",
       "      <td>0.588369</td>\n",
       "      <td>0.559361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.109341</td>\n",
       "      <td>0.604230</td>\n",
       "      <td>0.589740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.101964</td>\n",
       "      <td>0.595166</td>\n",
       "      <td>0.569757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.091316</td>\n",
       "      <td>0.605740</td>\n",
       "      <td>0.584157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.079203</td>\n",
       "      <td>0.620091</td>\n",
       "      <td>0.608992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>1.070777</td>\n",
       "      <td>0.626133</td>\n",
       "      <td>0.615967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.059380</td>\n",
       "      <td>0.619335</td>\n",
       "      <td>0.612469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.050123</td>\n",
       "      <td>0.629909</td>\n",
       "      <td>0.619888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.053879</td>\n",
       "      <td>0.617069</td>\n",
       "      <td>0.594304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.039411</td>\n",
       "      <td>0.635952</td>\n",
       "      <td>0.625556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.032358</td>\n",
       "      <td>0.642749</td>\n",
       "      <td>0.632915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.033085</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.019792</td>\n",
       "      <td>0.643505</td>\n",
       "      <td>0.630961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.016912</td>\n",
       "      <td>0.646526</td>\n",
       "      <td>0.636645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.009175</td>\n",
       "      <td>0.650302</td>\n",
       "      <td>0.637874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>0.993819</td>\n",
       "      <td>0.648792</td>\n",
       "      <td>0.639289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.990888</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.653150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.980763</td>\n",
       "      <td>0.654079</td>\n",
       "      <td>0.647318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>0.663142</td>\n",
       "      <td>0.658616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.976486</td>\n",
       "      <td>0.666163</td>\n",
       "      <td>0.657856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.969405</td>\n",
       "      <td>0.666918</td>\n",
       "      <td>0.658088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.975888</td>\n",
       "      <td>0.669184</td>\n",
       "      <td>0.658530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.965724</td>\n",
       "      <td>0.675227</td>\n",
       "      <td>0.666471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.943384</td>\n",
       "      <td>0.679758</td>\n",
       "      <td>0.675364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>0.947118</td>\n",
       "      <td>0.685801</td>\n",
       "      <td>0.680233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.961290</td>\n",
       "      <td>0.675227</td>\n",
       "      <td>0.664917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.938552</td>\n",
       "      <td>0.679758</td>\n",
       "      <td>0.678669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.939674</td>\n",
       "      <td>0.684290</td>\n",
       "      <td>0.680222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.940464</td>\n",
       "      <td>0.685801</td>\n",
       "      <td>0.677574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.918335</td>\n",
       "      <td>0.688822</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.920926</td>\n",
       "      <td>0.690332</td>\n",
       "      <td>0.685151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.916403</td>\n",
       "      <td>0.687311</td>\n",
       "      <td>0.683472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.936167</td>\n",
       "      <td>0.679758</td>\n",
       "      <td>0.671175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.903472</td>\n",
       "      <td>0.692598</td>\n",
       "      <td>0.689162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.907265</td>\n",
       "      <td>0.687311</td>\n",
       "      <td>0.679633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.901393</td>\n",
       "      <td>0.690332</td>\n",
       "      <td>0.684507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.900205</td>\n",
       "      <td>0.694109</td>\n",
       "      <td>0.687858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.913981</td>\n",
       "      <td>0.688822</td>\n",
       "      <td>0.679591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.887623</td>\n",
       "      <td>0.701662</td>\n",
       "      <td>0.696368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.894163</td>\n",
       "      <td>0.697885</td>\n",
       "      <td>0.691877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.882787</td>\n",
       "      <td>0.700906</td>\n",
       "      <td>0.699056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.706232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.876829</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.706033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>0.879276</td>\n",
       "      <td>0.704683</td>\n",
       "      <td>0.700121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.882563</td>\n",
       "      <td>0.702417</td>\n",
       "      <td>0.696009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.875158</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.705537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.867864</td>\n",
       "      <td>0.706949</td>\n",
       "      <td>0.702876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.865291</td>\n",
       "      <td>0.710725</td>\n",
       "      <td>0.709645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.867610</td>\n",
       "      <td>0.707704</td>\n",
       "      <td>0.703448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.860398</td>\n",
       "      <td>0.717523</td>\n",
       "      <td>0.714714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.857940</td>\n",
       "      <td>0.718278</td>\n",
       "      <td>0.714644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.856181</td>\n",
       "      <td>0.709970</td>\n",
       "      <td>0.706744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.849670</td>\n",
       "      <td>0.713746</td>\n",
       "      <td>0.712589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.846296</td>\n",
       "      <td>0.719033</td>\n",
       "      <td>0.717592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.846955</td>\n",
       "      <td>0.715257</td>\n",
       "      <td>0.714084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.845625</td>\n",
       "      <td>0.717523</td>\n",
       "      <td>0.716470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.845763</td>\n",
       "      <td>0.718278</td>\n",
       "      <td>0.717040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.849667</td>\n",
       "      <td>0.713746</td>\n",
       "      <td>0.708457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.842091</td>\n",
       "      <td>0.719033</td>\n",
       "      <td>0.719320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.840435</td>\n",
       "      <td>0.716767</td>\n",
       "      <td>0.716039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.837750</td>\n",
       "      <td>0.716012</td>\n",
       "      <td>0.714148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.842257</td>\n",
       "      <td>0.713746</td>\n",
       "      <td>0.708927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5100, training_loss=1.2120592483819699, metrics={'train_runtime': 1969.2409, 'train_samples_per_second': 266.565, 'train_steps_per_second': 16.671, 'total_flos': 2.16067820301312e+16, 'train_loss': 1.2120592483819699, 'epoch': 10.87})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DatasetDict\n",
    "dataset_path = 'RikoteMaster/isear_augmented'\n",
    "dataset_dict = load_dataset(dataset_path)\n",
    "\n",
    "dataset_dict = dataset_dict.remove_columns('Augmented')\n",
    "dataset_dict\n",
    "\n",
    "#charge the train datasetDict as a df\n",
    "df = dataset_dict['train'].to_pandas()\n",
    "df.head()\n",
    "#create id2label and label2id\n",
    "id2label = {i: label for i, label in enumerate(df['Emotion'].unique())}\n",
    "label2id = {label: i for i, label in enumerate(df['Emotion'].unique())}\n",
    "\n",
    "#apply label2id to the datasetDict\n",
    "dataset_dict = dataset_dict.map(lambda example: {'labels': label2id[example['Emotion']]}, remove_columns=['Emotion'])\n",
    "\n",
    "dataset_dict\n",
    "\n",
    "\"\"\"### Carga del tokenizador\"\"\"\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def tokenize_text(examples):\n",
    "    return tokenizer(examples[\"Text_processed\"], padding=\"max_length\")\n",
    "\n",
    "dataset_dict = dataset_dict.map(tokenize_text, batched=True)\n",
    "dataset_dict\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "\n",
    "    return {'eval_accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "peft_config  = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"value\"],\n",
    "        lora_dropout=0.8,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_objective(metrics):\n",
    "\n",
    "    return metrics['eval_accuracy'] + metrics['f1']\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 70\n",
    "\n",
    "output_dir = ''\n",
    "logging_steps = len(dataset_dict['train']) // batch_size\n",
    "args = TrainingArguments( output_dir=output_dir, \n",
    "                        num_train_epochs=epochs,\n",
    "                        learning_rate=0.000024509631236742206,\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        weight_decay=8.393030941902047e-05,\n",
    "                        evaluation_strategy = IntervalStrategy.STEPS,\n",
    "                        eval_steps = 50, # Evaluation and Save happens every 50 steps\n",
    "                        save_total_limit = 5,\n",
    "                        logging_steps=logging_steps,\n",
    "                        fp16=True,\n",
    "                        push_to_hub=False,\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model='accuracy')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=len(id2label), ignore_mismatched_sizes=True).to(device)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=dataset_dict['train'],\n",
    "                  eval_dataset=dataset_dict['validation'],\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer,\n",
    "                 callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659ca9262e224e7c9e82fee528fd8adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e18f5586a241df82064cadcd16fd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/RikoteMaster/Bert_best_params/commit/ec04e2fbf0d03816cb1ff2cc2272a90dbd3ecddc', commit_message='Upload model', commit_description='', oid='ec04e2fbf0d03816cb1ff2cc2272a90dbd3ecddc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"RikoteMaster/Bert_best_params\")\n",
    "trainer.push_to_hub(\"RikoteMaster/Bert_best_params_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
