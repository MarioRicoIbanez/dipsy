{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59519a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install bitsandbytes\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d20954",
   "metadata": {},
   "source": [
    "## Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c5741d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForSequenceClassification' from 'transformers' (/usr/local/lib/python3.8/dist-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Related to transformers and models\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m     18\u001b[0m     AutoTokenizer,\n\u001b[1;32m     19\u001b[0m     DataCollatorForSequenceClassification,\n\u001b[1;32m     20\u001b[0m     TrainingArguments,\n\u001b[1;32m     21\u001b[0m     Trainer,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataCollatorForSequenceClassification' from 'transformers' (/usr/local/lib/python3.8/dist-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "#General imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gdown \n",
    "import os\n",
    "import requests\n",
    "import string\n",
    "import re \n",
    "import shutil\n",
    "from utils import *\n",
    "\n",
    "\n",
    "#Related to transformers and models\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "#Result imports\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a395184",
   "metadata": {},
   "source": [
    "## Descarga del modelo a fine tunear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a37be3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhadresh-savani/bert-base-go-emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhadresh-savani/bert-base-go-emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = \"bhadresh-savani/bert-base-go-emotion\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTopClassification.from_pretrained(\"bhadresh-savani/bert-base-go-emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463d724",
   "metadata": {},
   "source": [
    "## Descarga y tratamiento de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58204f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fichero se encuentra en enlace externo\n",
    "url = \"https://raw.githubusercontent.com/PoorvaRane/Emotion-Detector/master/ISEAR.csv\"\n",
    "output_file = \"ISEAR.csv\"\n",
    "\n",
    "destination_folder = \"data\"\n",
    "#Si no existe la carpeta en la que queremos guardar los datos debemos crearla\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(output_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "shutil.move(output_file, f\"{destination_folder}/{output_file}\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = load_and_preprocess_data('./data/ISEAR.csv')\n",
    "df['Emotion'] = df['Emotion'].replace('guit', 'guilt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171df0",
   "metadata": {},
   "source": [
    "## Probamos el modelo a ver de cuantas clases dispone  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e15b181",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextClassificationPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mTextClassificationPipeline\u001b[49m(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, return_all_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m resultados \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuck you leave me alone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(resultados)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextClassificationPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "resultados = pipe(\"fuck you leave me alone\")\n",
    "print(resultados)\n",
    "accum = 0\n",
    "for elemento in resultados: \n",
    "    for cuenta in elemento:\n",
    "        accum = accum + 1 \n",
    "print(accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46ed0d",
   "metadata": {},
   "source": [
    "Como observamos en este modelo se tienen 28 clases (clásico del dataset de goemotion), ahora tenemos varias opciones, podemos intentar modificar esta ultima capa para poder trabajar con los logits, lo cual numéricamente es más estable o añadir una capa final de clasificación después de esta para clasificar entre las emociones de ekman que estamos utilizando en nuestro estudio. \n",
    "\n",
    "No sabemos el método que han usado para clasificar las emociones. Por tanto lo primero que vamos a hacer va a ser realizar un entrenamiento básico añadiendo una capa final para clasificar lo que nos interesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4505b",
   "metadata": {},
   "source": [
    "## Exploración del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)\n",
    "print(model.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f0456",
   "metadata": {},
   "source": [
    "El objeto `BertConfig` contiene los parámetros y configuraciones necesarios para el modelo BERT utilizado en la clasificación de secuencias con múltiples etiquetas. A continuación se presenta una explicación de los campos clave en la configuración:\n",
    "\n",
    "- `hidden_size` y `num_attention_heads`: Estos parámetros determinan la dimensionalidad de los vectores ocultos y el número de cabezas de atención en el modelo BERT, respectivamente.\n",
    "\n",
    "- `num_hidden_layers`: Indica el número de capas ocultas en el modelo. Aumentar este valor puede aumentar la capacidad del modelo para aprender representaciones más complejas.\n",
    "\n",
    "- `dropout`: Controla la tasa de abandono (dropout) aplicada a las salidas de las capas ocultas para evitar el sobreajuste y mejorar la generalización.\n",
    "\n",
    "- `initializer_range`: Especifica el rango de inicialización para los pesos del modelo.\n",
    "\n",
    "- `max_position_embeddings`: Define la longitud máxima permitida de las secuencias de entrada.\n",
    "\n",
    "- `vocab_size`: Indica el tamaño del vocabulario utilizado por el modelo.\n",
    "\n",
    "Estos campos son cruciales para configurar correctamente el modelo BERT en la clasificación de secuencias con múltiples etiquetas. Sin embargo, también debemos considerar otros aspectos como el modelo preentrenado, el conjunto de datos y los hiperparámetros específicos del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"I love this movie!\"\n",
    "inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Obtener los logits\n",
    "logits = outputs.logits\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f7d4d",
   "metadata": {},
   "source": [
    "Ahora queremos obtener cual es la clase que es más probable que se haya predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b118d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilidades = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Obtener la clase con la probabilidad más alta\n",
    "clase_predicha_idx = np.argmax(probabilidades.detach().numpy())\n",
    "clase_predicha = model.config.id2label[clase_predicha_idx]\n",
    "\n",
    "print(\"Clase predicha (índice):\", clase_predicha_idx)\n",
    "print(\"Clase predicha (texto):\", clase_predicha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c1ca8",
   "metadata": {},
   "source": [
    "## Creación de la clase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef29e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiModelo(nn.Module):\n",
    "    def __init__(self, modelo_base, num_clases):\n",
    "        super(MiModelo, self).__init__()\n",
    "        \n",
    "        # Definimos el modelo base que estará preentrenado\n",
    "        self.modelo_base = modelo_base\n",
    "\n",
    "        # Capa de dropout para evitar sobreajuste, con probabilidad de dropout de 0.5\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Capa totalmente conectada que transforma la salida del modelo base en una salida de tamaño 14\n",
    "        self.fc1 = nn.Linear(self.modelo_base.config.hidden_size, 14)\n",
    "        \n",
    "        # Función de activación ReLU\n",
    "        self.activacion = nn.ReLU()\n",
    "        \n",
    "        # Segunda capa totalmente conectada que transforma la salida de tamaño 14 a la cantidad de clases\n",
    "        self.fc2 = nn.Linear(14, num_clases)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pasamos los inputs a través del modelo base\n",
    "        outputs = self.modelo_base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Tomamos la salida del modelo base (output del token de clasificación)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Aplicamos dropout a la salida del modelo base\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Pasamos la salida por la primera capa completamente conectada\n",
    "        hidden = self.fc1(pooled_output)\n",
    "        \n",
    "        # Aplicamos la función de activación\n",
    "        hidden = self.activacion(hidden)\n",
    "        \n",
    "        # Pasamos la salida por la segunda capa completamente conectada para obtener los logits finales\n",
    "        logits = self.fc2(hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def training_step(self, inputs, targets):\n",
    "        # Obtenemos los logits pasando los inputs por el modelo\n",
    "        logits = self.forward(inputs.input_ids, inputs.attention_mask)\n",
    "        \n",
    "        # Definimos la función de pérdida (Cross Entropy Loss)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Calculamos la pérdida\n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = 7\n",
    "mi_modelo = MiModelo(modelo_base=model, num_clases=num_clases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f10deb",
   "metadata": {},
   "source": [
    "## Analisis de mejoras usando Peft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855fdee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
