{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b86b964",
   "metadata": {},
   "source": [
    "\n",
    " ### Creating synthetic dataset\n",
    " The process is going to be, create a good prompt for llama-2-40b-chat-hf. And infere over the 63k prompt in order to make a emotion-reason dataset. By this way, we can try to make a LLM that is capable to explain emotions and may make better predictions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef1951",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from numpy import save, asarray\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3906b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"RikoteMaster/Emotion_Recognition_4_llama2_chat\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-sentiment-analyzer\"\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# Loading model\n",
    "################################################################################\n",
    "\n",
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49c6dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/Emotion_Recognition_4_llama2_chat\")\n",
    "ds = ds['train']\n",
    "ds = pd.DataFrame.from_dict(ds)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "def bigger_formatting(text, label):\n",
    "    prompt = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\n",
    "\n",
    "    Anger, Joy, Sadness, Guilt, Shame, fear or disgust. \n",
    "    \n",
    "    Sentence: {text} Emotion: {label} [/INST] Please answer only with the explanation of the Emotion. For example, in the input Sentence: I feel sad when my mother leaves home. Emotion: Sadness. You should answer EXPLANATION: In this sentence the feeling of sadness is due to the person is not going to see her mother in a period of time. \"\"\"\n",
    "    return prompt\n",
    "\n",
    "for index, row in ds.iterrows():\n",
    "    ds.loc[index, 'text'] = bigger_formatting(row['Text_processed'], row['Emotion'])\n",
    "\n",
    "print(ds['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240ab68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prediction(text):\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=700)\n",
    "    result = pipe(text)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Set the total number of iterations (progress total)\n",
    "total_iterations = len(ds)\n",
    "\n",
    "# Create a tqdm progress bar\n",
    "for index, row in tqdm(ds.iterrows(), total=total_iterations, desc=\"Generating Predictions\"):\n",
    "    prediction_aux = prediction(row['text'])\n",
    "    predictions.append(prediction_aux)\n",
    "    if index%100 == 0:\n",
    "        print(prediction_aux)\n",
    "        save('data_aux.npy', asarray(predictions))\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import save, load\n",
    "\n",
    "#save data\n",
    "save('data.npy', asarray(predictions))\n",
    "\n",
    "predicted = load('data.npy')\n",
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a4594",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save, load\n",
    "\n",
    "#save data\n",
    "save('data.npy', asarray(predictions))\n",
    "\n",
    "predicted = load('data.npy')\n",
    "predicted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369191d",
   "metadata": {},
   "source": [
    "### Analyzing quality of explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cc75c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 43386 into shape (61463,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRikoteMaster/Emotion_Recognition_4_llama2_chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/format.py:832\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    830\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 832\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 43386 into shape (61463,)"
     ]
    }
   ],
   "source": [
    "from numpy import save, load\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "predicted = load('data.npy')\n",
    "ds = load_dataset(\"RikoteMaster/Emotion_Recognition_4_llama2_chat\")\n",
    "ds = ds['train']\n",
    "ds = pd.DataFrame.from_dict(ds)\n",
    "print(len(ds),len(predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_Text_processed = ds['Text_processed']\n",
    "ds_Emotion = ds['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "counter_0 = 0\n",
    "explanation_concat = []\n",
    "i = 0 \n",
    "axis_drop = []\n",
    "for sentence in predicted:\n",
    "    try:\n",
    "        explanation = sentence.lower().split('explanation:')[2]\n",
    "        if len(explanation) == 0 :\n",
    "            counter_0 += 1\n",
    "            axis_drop.append(i)\n",
    "\n",
    "        else: \n",
    "            explanation_concat.append(explanation)\n",
    "            \n",
    "    except:\n",
    "        counter += 1\n",
    "        axis_drop.append(i)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(counter, counter_0, i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_Text_processed = ds_Text_processed.drop(axis_drop).reset_index(drop=True)\n",
    "ds_Emotion = ds_Emotion.drop(axis_drop).reset_index(drop=True)\n",
    "\n",
    "print(len(ds_Text_processed), len(explanation_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be26b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_cleaned_length = []\n",
    "counter_10 = 0\n",
    "axis_drop = []\n",
    "for i in range(0,len(explanation_concat)): \n",
    "    if len(explanation_concat[i].split()) >= 10:\n",
    "        explanation_cleaned_length.append(explanation_concat[i])\n",
    "    else:\n",
    "        counter_10 += 1\n",
    "        axis_drop.append(i)\n",
    "        \n",
    "ds_Text_processed = ds_Text_processed.drop(axis_drop).reset_index(drop=True)\n",
    "ds_Emotion = ds_Emotion.drop(axis_drop).reset_index(drop=True)\n",
    "\n",
    "print(len(ds_Text_processed), len(explanation_cleaned_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_is = 0\n",
    "explanation_cleaned_is = []\n",
    "\n",
    "i = 0\n",
    "axis_drop = []\n",
    "\n",
    "for sentence in explanation_cleaned_length:\n",
    "    if 'is:' in sentence:\n",
    "        counter_is += 1\n",
    "        axis_drop.append(i)\n",
    "    else: \n",
    "        explanation_cleaned_is.append(sentence)\n",
    "    i += 1\n",
    "\n",
    "ds_Text_processed = ds_Text_processed.drop(axis_drop).reset_index(drop=True)\n",
    "ds_Emotion = ds_Emotion.drop(axis_drop).reset_index(drop=True)\n",
    "\n",
    "print(len(ds_Text_processed), len(explanation_cleaned_is))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(explanation_cleaned_is),100):\n",
    "    print(explanation_cleaned_is[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_emotion = 0\n",
    "explanation_cleaned_emotion = []\n",
    "\n",
    "i = 0\n",
    "axis_drop = []\n",
    "\n",
    "for sentence in explanation_cleaned_is:\n",
    "    if 'emotion:' in sentence:\n",
    "        counter_emotion += 1\n",
    "        axis_drop.append(i)\n",
    "    else: \n",
    "        explanation_cleaned_emotion.append(sentence)\n",
    "    i += 1\n",
    "        \n",
    "        \n",
    "ds_Text_processed = ds_Text_processed.drop(axis_drop).reset_index(drop=True)\n",
    "ds_Emotion = ds_Emotion.drop(axis_drop).reset_index(drop=True)\n",
    "\n",
    "print(len(ds_Text_processed), len(explanation_cleaned_emotion))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0084e41",
   "metadata": {},
   "source": [
    "### Now creating a new dataset with the new formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a61c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_explanation = pd.DataFrame(explanation_cleaned_emotion, columns=['Explanation'])\n",
    "\n",
    "# Create a mask for rows that start with '\\n'\n",
    "mask = ~ds_explanation['Explanation'].str.startswith('\\n')\n",
    "\n",
    "# Apply the mask to the DataFrames\n",
    "ds_explanation = ds_explanation[mask]\n",
    "ds_Text_processed = ds_Text_processed[mask]  # Apply the same mask to ds_Text_processed\n",
    "ds_Emotion = ds_Emotion[mask]  # Apply the same mask to ds_Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_explanation = ds_explanation.reset_index(drop=True)\n",
    "ds_Text_processed = ds_Text_processed.reset_index(drop=True)\n",
    "ds_Emotion = ds_Emotion.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fcd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_explanation['Text_processed'] = ds_Text_processed\n",
    "ds_explanation['Emotion'] = ds_Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24469a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c386295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigger_formatting(text, emotion, explanation):\n",
    "    formatted_text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\n",
    "\n",
    "    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\n",
    "    \n",
    "    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\n",
    "\n",
    "    Sentence: {text} [/INST] Emotion: {emotion} Explanation: {explanation} <s>\"\"\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Apply the formatting function to each row and add to a list\n",
    "formatted_rows = [bigger_formatting(text, emotion, explanation) for text, emotion, explanation in zip(ds_Text_processed, ds_Emotion, ds_explanation['Explanation'])]\n",
    "\n",
    "# Add the list as a new column 'Formatted_Text' to the ds_explanation DataFrame\n",
    "ds_explanation['text'] = formatted_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88539a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_explanation['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "ds_explanation = Dataset.from_pandas(ds_explanation)\n",
    "ds_explanation.push_to_hub('RikoteMaster/llama2_classifying_and_explainning_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1349f7",
   "metadata": {},
   "source": [
    "### Re-trainning with explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1edd25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"RikoteMaster/llama2_classifying_and_explainning_v2\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-sentiment-analyzer\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 4\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 32\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-6\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af969eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dae690",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I hate football\"\n",
    "text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ee2c0",
   "metadata": {},
   "source": [
    "Making inference in order to try if the reasonning concept improves the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542ae3c",
   "metadata": {},
   "source": [
    "### Inference over isear validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4be4f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/isear_for_llama2\")\n",
    "\n",
    "texts = ds['validation']['Text_processed']\n",
    "labels = ds['validation']['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad07723",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_detection = []\n",
    "corrects = 0\n",
    "exceptions = 0\n",
    "for sentence, label in zip(texts, labels):\n",
    "    text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=len(tokenizer(text)) + 200)\n",
    "    result = pipe(text)\n",
    "    try:\n",
    "        detected = result[0]['generated_text'].split('Emotion:')[2].split()[0]\n",
    "        if label != detected:\n",
    "            wrong_detection.append(str(result) + \" THE TRUE LABEL IS \"+ label)\n",
    "        else:\n",
    "            corrects += 1\n",
    "    except:\n",
    "        \n",
    "        wrong_detection.append(str(result) + \" THE TRUE LABEL IS \" + label)\n",
    "        exceptions += 1 \n",
    "        \n",
    "\n",
    "\n",
    "print(corrects/len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in wrong_detection:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272c00b",
   "metadata": {},
   "source": [
    "### Inference over isear test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3e9de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/isear_for_llama2\")\n",
    "\n",
    "texts = ds['test']['Text_processed']\n",
    "labels = ds['test']['Emotion']\n",
    "\n",
    "\n",
    "\n",
    "wrong_detection = []\n",
    "corrects = 0\n",
    "exceptions = 0\n",
    "for sentence, label in zip(texts, labels):\n",
    "    text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=len(tokenizer(text)) + 200)\n",
    "    result = pipe(text)\n",
    "    try:\n",
    "        detected = result[0]['generated_text'].split('Emotion:')[2].split()[0]\n",
    "        if label != detected:\n",
    "            wrong_detection.append(str(result) + \" THE TRUE LABEL IS \"+ label)\n",
    "        else:\n",
    "            corrects += 1\n",
    "    except:\n",
    "        \n",
    "        wrong_detection.append(str(result) + \" THE TRUE LABEL IS \" + label)\n",
    "        exceptions += 1 \n",
    "        \n",
    "\n",
    "\n",
    "print(corrects/len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc14ab",
   "metadata": {},
   "source": [
    "### CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c63ed",
   "metadata": {},
   "source": [
    "After seeing the results, I think that the not stabilized dataset could create problems in detecting emotions, so the next step is stabilize the classes of the dataset. \n",
    "\n",
    "What is supposed to happen to the model when it has an unstable dataset?\n",
    "\n",
    "I think that the result on the inference may be more probably for the classes with more samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f341d5",
   "metadata": {},
   "source": [
    "### Retrainning with dataset cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72547d05",
   "metadata": {},
   "source": [
    "In the process of cleaning the dataset, I have selected the largest 2884 sentences from each class so as to balance the classes and choose the higher-quality sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6cb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3e40b72",
   "metadata": {},
   "source": [
    "# DO NOT RUN ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454f333",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/llama2_classifying_and_explainning_v2\")\n",
    "ds = ds['train']\n",
    "ds = pd.DataFrame.from_dict(ds)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ds_pandas = pd.DataFrame.from_dict(ds['train'])\n",
    "\n",
    "ds_pandas.Emotion.value_counts()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ds_pandas is your dataset\n",
    "\n",
    "# Create a new DataFrame to store the reduced dataset\n",
    "reduced_dataset = pd.DataFrame(columns=ds_pandas.columns)\n",
    "\n",
    "# Iterate over each emotion\n",
    "for emotion in ds_pandas['Emotion'].unique():\n",
    "    # Get the subset of the dataset for the current emotion\n",
    "    subset = ds_pandas[ds_pandas['Emotion'] == emotion]\n",
    "\n",
    "    # Sort the subset by the 'Text_processed' column in descending order\n",
    "    sorted_subset = subset.sort_values(by='Text_processed', ascending=False)\n",
    "\n",
    "    # Select the top 2884 samples from the sorted subset\n",
    "    selected_samples = sorted_subset.head(2884)\n",
    "\n",
    "    # Append the selected samples to the reduced_dataset\n",
    "    reduced_dataset = reduced_dataset.append(selected_samples, ignore_index=True)\n",
    "\n",
    "# The reduced_dataset now contains 2884 samples for each emotion, with the largest Text_processed values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ds_pandas is your dataset\n",
    "\n",
    "# Remove duplicated sentences\n",
    "ds_pandas_unique = ds_pandas.drop_duplicates(subset='Text_processed')\n",
    "\n",
    "# Get value counts with Emotion label\n",
    "value_counts_by_emotion = ds_pandas_unique['Emotion'].value_counts()\n",
    "\n",
    "print(value_counts_by_emotion)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming ds_pandas is your dataset\n",
    "\n",
    "# Create a new DataFrame to store the selected sentences\n",
    "selected_sentences = pd.DataFrame(columns=ds_pandas.columns)\n",
    "\n",
    "# Iterate over each emotion\n",
    "for emotion in ds_pandas_unique['Emotion'].unique():\n",
    "    # Get the subset of the dataset for the current emotion\n",
    "    subset = ds_pandas_unique[ds_pandas_unique['Emotion'] == emotion]\n",
    "\n",
    "    # Sort the subset by the 'Text_processed' column in descending order\n",
    "    sorted_subset = subset.sort_values(by='Text_processed', ascending=False)\n",
    "\n",
    "    # Select the top 2000 sentences from the sorted subset\n",
    "    selected_samples = sorted_subset.head(2000)\n",
    "\n",
    "    # Append the selected samples to the selected_sentences DataFrame\n",
    "    selected_sentences = selected_sentences.append(selected_samples, ignore_index=True)\n",
    "\n",
    "# The selected_sentences DataFrame now contains the 2000 largest sentences for each emotion\n",
    "\n",
    "def bigger_formatting(text, label):\n",
    "    prompt = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\n",
    "\n",
    "    Anger, Joy, Sadness, Guilt, Shame, fear or disgust. \n",
    "    \n",
    "    Sentence: {text} Emotion: {label} [/INST] Please answer only with the explanation of the Emotion. For example, in the input Sentence: I feel sad when my mother leaves home. Emotion: Sadness. You should answer EXPLANATION: In this sentence the feeling of sadness is due to the person is not going to see her mother in a period of time. \"\"\"\n",
    "    return prompt\n",
    "\n",
    "for index, row in selected_sentences.iterrows():\n",
    "    selected_sentences.loc[index, 'text'] = bigger_formatting(row['Text_processed'], row['Emotion'])\n",
    "\n",
    "ds = Dataset.from_pandas(selected_sentences)\n",
    "\n",
    "ds.push_to_hub('RikoteMaster/llama2_classifying_and_explainning_v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015d0fa",
   "metadata": {},
   "source": [
    "### REtrainning with new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a03a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efe759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"RikoteMaster/llama2_classifying_and_explainning_v4\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-sentiment-analyzer\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results_selected\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 12\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 32\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-6\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73059f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0be004f",
   "metadata": {},
   "source": [
    "### Re running last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"RikoteMaster/llama2_classifying_and_explainning_v4\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-sentiment-analyzer\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ae160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95e2c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/isear_for_llama2\")\n",
    "\n",
    "texts = ds['test']['Text_processed']\n",
    "labels = ds['test']['Emotion']\n",
    "\n",
    "\n",
    "label_detection = []\n",
    "wrong_detection = []\n",
    "corrects = 0\n",
    "exceptions = 0\n",
    "for sentence, label in zip(texts, labels):\n",
    "    text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=len(tokenizer(text)) + 200)\n",
    "    result = pipe(text)\n",
    "    try:\n",
    "        detected = result[0]['generated_text'].split('Emotion:')[2].split()[0]\n",
    "        if label != detected:\n",
    "            wrong_detection.append(str(result) + \" THE TRUE LABEL IS \"+ label)\n",
    "            label_detection.append(detected)\n",
    "        else:\n",
    "            corrects += 1\n",
    "    except:\n",
    "        \n",
    "        wrong_detection.append(str(result) + \" THE TRUE LABEL IS \" + label)\n",
    "        label_detection.append(detected)\n",
    "        exceptions += 1 \n",
    "        \n",
    "\n",
    "\n",
    "print(corrects/len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db916429",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(label_detection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca97c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I hate beeing bad treated when I went shopping with my lovely sister\"\n",
    "text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000302c",
   "metadata": {},
   "source": [
    "The issue with this dataset is that due to the limited number of sentences, it becomes challenging to precisely determine which labels the model should utilize, even after training it for numerous epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d5f19",
   "metadata": {},
   "source": [
    "Now I am going to retrain with the dataset that has several repetitions so as to analyze if we can augment the accuracy by reducing the imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa740e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-31 09:54:58,960] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac789962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"RikoteMaster/llama2_classifying_and_explainning_v3\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-sentiment-analyzer\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results_selected\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 32\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 3e-6\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d4c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|| 587/587 [00:00<00:00, 2.95MB/s]\n",
      "Downloading data files:   0%|                                                                                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                                               | 0.00/6.80M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|| 6.80M/6.80M [00:01<00:00, 5.22MB/s]\u001b[A\n",
      "Downloading data files: 100%|| 1/1 [00:01<00:00,  1.31s/it]\n",
      "Extracting data files: 100%|| 1/1 [00:00<00:00, 1179.50it/s]\n",
      "Generating train split: 100%|| 20188/20188 [00:00<00:00, 311784.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 2/2 [00:09<00:00,  4.83s/it]\n",
      "/usr/local/lib/python3.8/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|| 20188/20188 [00:01<00:00, 11893.61 examples/s]\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3155' max='3155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3155/3155 2:18:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.966500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>2.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.204800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>2.014600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.918100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.816800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.726300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.714700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.637800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.697600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.574200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.575700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.570600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.580600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.552900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.599700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.606500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.602200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>0.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>0.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.466500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "042cda75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Input length of input_ids is 245, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 204, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 266, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 324, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 220, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 207, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 204, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 226, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 217, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 206, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 210, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 221, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 209, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 213, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 233, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 241, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 227, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 220, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 206, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 209, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 234, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 222, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 227, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 214, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 228, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 317, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 211, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 207, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 222, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 215, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 210, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 222, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 240, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 262, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 221, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 212, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 274, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 213, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 333, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 232, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 206, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 204, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 243, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 210, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 241, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 225, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 263, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 219, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 264, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 256, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 218, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 222, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 208, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 205, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 220, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "Input length of input_ids is 259, but `max_length` is set to 202. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6157530601383715\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"RikoteMaster/isear_for_llama2\")\n",
    "\n",
    "texts = ds['test']['Text_processed']\n",
    "labels = ds['test']['Emotion']\n",
    "\n",
    "\n",
    "label_detection = []\n",
    "wrong_detection = []\n",
    "corrects = 0\n",
    "exceptions = 0\n",
    "for sentence, label in zip(texts, labels):\n",
    "    text = f\"\"\"<s>[INST] In this task, you will be performing a classification exercise aimed at identifying the underlying emotion conveyed by a given sentence. The emotions to consider are as follows:\\n\\n    Anger, Joy, Sadness, Guilt, Shame, Fear, or Disgust\\n    \\n    You have to first classify the emotion using Emotion: and later, you will have to provide an explanation of why you think the sentence is expressing that emotion, using Explanation:\\n\\n    Sentence: {sentence} [/INST]\"\"\"\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=len(tokenizer(text)) + 200)\n",
    "    result = pipe(text)\n",
    "    try:\n",
    "        detected = result[0]['generated_text'].split('Emotion:')[2].split()[0]\n",
    "        if label != detected:\n",
    "            wrong_detection.append(str(result) + \" THE TRUE LABEL IS \"+ label)\n",
    "            label_detection.append(detected)\n",
    "        else:\n",
    "            corrects += 1\n",
    "    except:\n",
    "        \n",
    "        wrong_detection.append(str(result) + \" THE TRUE LABEL IS \" + label)\n",
    "        label_detection.append(detected)\n",
    "        exceptions += 1 \n",
    "        \n",
    "\n",
    "\n",
    "print(corrects/len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9231d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
