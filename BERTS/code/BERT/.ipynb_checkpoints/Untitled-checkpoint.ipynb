{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59519a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install bitsandbytes\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d20954",
   "metadata": {},
   "source": [
    "## Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c5741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.8/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib'), PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/compat/lib')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/lib/python3.8/dist-packages/torch/lib:/usr/local/lib/python3.8/dist-packages/torch_tensorrt/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"/tmp/vscode-ssh-auth-4f228fc8-df9d-40e7-9a20-5e6a46d772d9.sock\",\"/root/.gnupg/S.gpg-agent\"]')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/usr/local/lib/python3.8/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#General imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import gdown \n",
    "import os\n",
    "import requests\n",
    "import string\n",
    "import re \n",
    "import shutil\n",
    "from utils import *\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#Related to transformers and models\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "#Result imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#GENERAL CONSTANTS\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a395184",
   "metadata": {},
   "source": [
    "## Descarga del modelo a fine tunear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a37be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bhadresh-savani/bert-base-go-emotion\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "modelo_base  = AutoModelForSequenceClassification.from_pretrained(\"bhadresh-savani/bert-base-go-emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463d724",
   "metadata": {},
   "source": [
    "## Descarga y tratamiento de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58204f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joy</td>\n",
       "      <td>On days when I feel close to my partner and ot...</td>\n",
       "      <td>on days when i feel close to my partner and ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear</td>\n",
       "      <td>Every time I imagine that someone I love or I ...</td>\n",
       "      <td>every time i imagine that someone i love or i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anger</td>\n",
       "      <td>When I had been obviously unjustly treated and...</td>\n",
       "      <td>when i had been obviously unjustly treated and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "      <td>When I think about the short time that we live...</td>\n",
       "      <td>when i think about the short time that we live...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disgust</td>\n",
       "      <td>At a gathering I found myself involuntarily si...</td>\n",
       "      <td>at a gathering i found myself involuntarily si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>shame</td>\n",
       "      <td>Two years back someone invited me to be the tu...</td>\n",
       "      <td>two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7512</th>\n",
       "      <td>shame</td>\n",
       "      <td>I had taken the responsibility to do something...</td>\n",
       "      <td>i had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>fear</td>\n",
       "      <td>I was at home and I heard a loud sound of spit...</td>\n",
       "      <td>i was at home and i heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7514</th>\n",
       "      <td>guilt</td>\n",
       "      <td>I did not do the homework that the teacher had...</td>\n",
       "      <td>i did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>fear</td>\n",
       "      <td>I had shouted at my younger brother and he was...</td>\n",
       "      <td>i had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7516 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text  \\\n",
       "0         joy  On days when I feel close to my partner and ot...   \n",
       "1        fear  Every time I imagine that someone I love or I ...   \n",
       "2       anger  When I had been obviously unjustly treated and...   \n",
       "3     sadness  When I think about the short time that we live...   \n",
       "4     disgust  At a gathering I found myself involuntarily si...   \n",
       "...       ...                                                ...   \n",
       "7511    shame  Two years back someone invited me to be the tu...   \n",
       "7512    shame  I had taken the responsibility to do something...   \n",
       "7513     fear  I was at home and I heard a loud sound of spit...   \n",
       "7514    guilt  I did not do the homework that the teacher had...   \n",
       "7515     fear  I had shouted at my younger brother and he was...   \n",
       "\n",
       "                                         Text_processed  \n",
       "0     on days when i feel close to my partner and ot...  \n",
       "1     every time i imagine that someone i love or i ...  \n",
       "2     when i had been obviously unjustly treated and...  \n",
       "3     when i think about the short time that we live...  \n",
       "4     at a gathering i found myself involuntarily si...  \n",
       "...                                                 ...  \n",
       "7511  two years back someone invited me to be the tu...  \n",
       "7512  i had taken the responsibility to do something...  \n",
       "7513  i was at home and i heard a loud sound of spit...  \n",
       "7514  i did not do the homework that the teacher had...  \n",
       "7515  i had shouted at my younger brother and he was...  \n",
       "\n",
       "[7516 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fichero se encuentra en enlace externo\n",
    "url = \"https://raw.githubusercontent.com/PoorvaRane/Emotion-Detector/master/ISEAR.csv\"\n",
    "output_file = \"ISEAR.csv\"\n",
    "\n",
    "destination_folder = \"data\"\n",
    "#Si no existe la carpeta en la que queremos guardar los datos debemos crearla\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(output_file, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "shutil.move(output_file, f\"{destination_folder}/{output_file}\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = load_and_preprocess_data('./data/ISEAR.csv')\n",
    "df['Emotion'] = df['Emotion'].replace('guit', 'guilt')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91171df0",
   "metadata": {},
   "source": [
    "## Probamos el modelo a ver de cuantas clases dispone  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e15b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'admiration', 'score': 0.0022283862344920635}, {'label': 'amusement', 'score': 0.00207656342536211}, {'label': 'anger', 'score': 0.6765081286430359}, {'label': 'annoyance', 'score': 0.11713651567697525}, {'label': 'approval', 'score': 0.006938391365110874}, {'label': 'caring', 'score': 0.008048061281442642}, {'label': 'confusion', 'score': 0.0015102234901860356}, {'label': 'curiosity', 'score': 0.0022445875220000744}, {'label': 'desire', 'score': 0.0015093624824658036}, {'label': 'disappointment', 'score': 0.018241364508867264}, {'label': 'disapproval', 'score': 0.016932494938373566}, {'label': 'disgust', 'score': 0.04833042994141579}, {'label': 'embarrassment', 'score': 0.0041035753674805164}, {'label': 'excitement', 'score': 0.002686750376597047}, {'label': 'fear', 'score': 0.0032824971713125706}, {'label': 'gratitude', 'score': 0.0020939591340720654}, {'label': 'grief', 'score': 0.002857409417629242}, {'label': 'joy', 'score': 0.0016693678917363286}, {'label': 'love', 'score': 0.0010952280135825276}, {'label': 'nervousness', 'score': 0.002996481955051422}, {'label': 'optimism', 'score': 0.0016878662863746285}, {'label': 'pride', 'score': 0.0007378275040537119}, {'label': 'realization', 'score': 0.003398278495296836}, {'label': 'relief', 'score': 0.0007681022398173809}, {'label': 'remorse', 'score': 0.0033701001666486263}, {'label': 'sadness', 'score': 0.016302889212965965}, {'label': 'surprise', 'score': 0.003571051638573408}, {'label': 'neutral', 'score': 0.04767394810914993}]]\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = TextClassificationPipeline(model=modelo_base, tokenizer=tokenizer, return_all_scores=True)\n",
    "resultados = pipe(\"fuck you leave me alone\")\n",
    "print(resultados)\n",
    "accum = 0\n",
    "for elemento in resultados: \n",
    "    for cuenta in elemento:\n",
    "        accum = accum + 1 \n",
    "print(accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46ed0d",
   "metadata": {},
   "source": [
    "Como observamos en este modelo se tienen 28 clases (clÃ¡sico del dataset de goemotion), ahora tenemos varias opciones, podemos intentar modificar esta ultima capa para poder trabajar con los logits, lo cual numÃ©ricamente es mÃ¡s estable o aÃ±adir una capa final de clasificaciÃ³n despuÃ©s de esta para clasificar entre las emociones de ekman que estamos utilizando en nuestro estudio. \n",
    "\n",
    "No sabemos el mÃ©todo que han usado para clasificar las emociones. Por tanto lo primero que vamos a hacer va a ser realizar un entrenamiento bÃ¡sico aÃ±adiendo una capa final para clasificar lo que nos interesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4505b",
   "metadata": {},
   "source": [
    "## ExploraciÃ³n del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bhadresh-savani/bert-base-go-emotion\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMultilabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(modelo_base.config)\n",
    "print(modelo_base.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f0456",
   "metadata": {},
   "source": [
    "El objeto `BertConfig` contiene los parÃ¡metros y configuraciones necesarios para el modelo BERT utilizado en la clasificaciÃ³n de secuencias con mÃºltiples etiquetas. A continuaciÃ³n se presenta una explicaciÃ³n de los campos clave en la configuraciÃ³n:\n",
    "\n",
    "- `hidden_size` y `num_attention_heads`: Estos parÃ¡metros determinan la dimensionalidad de los vectores ocultos y el nÃºmero de cabezas de atenciÃ³n en el modelo BERT, respectivamente.\n",
    "\n",
    "- `num_hidden_layers`: Indica el nÃºmero de capas ocultas en el modelo. Aumentar este valor puede aumentar la capacidad del modelo para aprender representaciones mÃ¡s complejas.\n",
    "\n",
    "- `dropout`: Controla la tasa de abandono (dropout) aplicada a las salidas de las capas ocultas para evitar el sobreajuste y mejorar la generalizaciÃ³n.\n",
    "\n",
    "- `initializer_range`: Especifica el rango de inicializaciÃ³n para los pesos del modelo.\n",
    "\n",
    "- `max_position_embeddings`: Define la longitud mÃ¡xima permitida de las secuencias de entrada.\n",
    "\n",
    "- `vocab_size`: Indica el tamaÃ±o del vocabulario utilizado por el modelo.\n",
    "\n",
    "Estos campos son cruciales para configurar correctamente el modelo BERT en la clasificaciÃ³n de secuencias con mÃºltiples etiquetas. Sin embargo, tambiÃ©n debemos considerar otros aspectos como el modelo preentrenado, el conjunto de datos y los hiperparÃ¡metros especÃ­ficos del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc9d005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2273, -4.7028, -5.9401, -5.2675, -3.2153, -4.5923, -5.4052, -4.9632,\n",
      "         -4.6493, -5.4046, -5.5101, -6.3988, -6.0861, -3.6266, -6.5459, -4.0359,\n",
      "         -7.2671, -2.7496,  1.5563, -7.0032, -4.6198, -6.0098, -4.3959, -6.7773,\n",
      "         -6.5881, -5.3570, -5.3487, -3.5594]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "texto = \"I love this movie!\"\n",
    "inputs = tokenizer(texto, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = modelo_base(**inputs)\n",
    "\n",
    "# Obtener los logits\n",
    "logits = outputs.logits\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f7d4d",
   "metadata": {},
   "source": [
    "Ahora queremos obtener cual es la clase que es mÃ¡s probable que se haya predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b118d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha (Ã­ndice): 18\n",
      "Clase predicha (texto): love\n"
     ]
    }
   ],
   "source": [
    "probabilidades = torch.softmax(logits, dim=1)\n",
    "\n",
    "# Obtener la clase con la probabilidad mÃ¡s alta\n",
    "clase_predicha_idx = np.argmax(probabilidades.detach().numpy())\n",
    "clase_predicha = modelo_base.config.id2label[clase_predicha_idx]\n",
    "\n",
    "print(\"Clase predicha (Ã­ndice):\", clase_predicha_idx)\n",
    "print(\"Clase predicha (texto):\", clase_predicha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311c1ca8",
   "metadata": {},
   "source": [
    "## CreaciÃ³n de la clase  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef29e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MiModelo(nn.Module):\n",
    "    def __init__(self, modelo_base, num_clases):\n",
    "        super(MiModelo, self).__init__()\n",
    "        \n",
    "        # Definimos el modelo base que estarÃ¡ preentrenado\n",
    "        self.modelo_base = modelo_base\n",
    "\n",
    "        # Capa de dropout para evitar sobreajuste, con probabilidad de dropout de 0.5\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Capa totalmente conectada que transforma la salida del modelo base en una salida de tamaÃ±o 14\n",
    "        self.fc1 = nn.Linear(28, 14)\n",
    "        \n",
    "        # FunciÃ³n de activaciÃ³n ReLU\n",
    "        self.activacion = nn.ReLU()\n",
    "        \n",
    "        # Segunda capa totalmente conectada que transforma la salida de tamaÃ±o 14 a la cantidad de clases\n",
    "        self.fc2 = nn.Linear(14, num_clases)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pasamos los inputs a travÃ©s del modelo base\n",
    "        outputs = self.modelo_base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Tomamos la salida del modelo base\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        # Aplicamos dropout a la salida del modelo base\n",
    "        pooled_output = self.dropout(last_hidden_state)\n",
    "\n",
    "        # Pasamos la salida por la primera capa completamente conectada\n",
    "        hidden = self.fc1(pooled_output)\n",
    "\n",
    "        # Aplicamos la funciÃ³n de activaciÃ³n\n",
    "        hidden = self.activacion(hidden)\n",
    "\n",
    "        # Pasamos la salida por la segunda capa completamente conectada para obtener los logits finales\n",
    "        logits = self.fc2(hidden)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def training_step(self, inputs, targets):\n",
    "        # Obtenemos los logits pasando los inputs por el modelo\n",
    "        logits = self.forward(inputs.input_ids, inputs.attention_mask)\n",
    "        \n",
    "        # Definimos la funciÃ³n de pÃ©rdida (Cross Entropy Loss)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Calculamos la pÃ©rdida\n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d941c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = 7\n",
    "mi_modelo = MiModelo(modelo_base=modelo_base, num_clases=num_clases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3d7ac",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe78600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df['Text_processed'].tolist()\n",
    "labels = df['Emotion'].tolist()\n",
    "\n",
    "# Convertir las etiquetas a nÃºmeros\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "# tokenizar inputs\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    inputs,\n",
    "    padding='longest', \n",
    "    truncation=True, \n",
    "    max_length=512, \n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "967f61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear conjunto de datos\n",
    "dataset = MyDataset(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4829c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d71399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89b4b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clases = len(le.classes_)\n",
    "model = MiModelo(modelo_base, num_clases)\n",
    "\n",
    "total_layers = len(list(model.modelo_base.parameters()))\n",
    "unfrozen_layers = total_layers - 2  # Descongelar las Ãºltimas dos capas\n",
    "\n",
    "for i, param in enumerate(model.modelo_base.parameters()):\n",
    "    if i >= unfrozen_layers:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2b1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a660ec3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1088397419.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f'epoch: {epoch} train_loss {train_loss} val_loss {val_loss} val_accuracy {val_accuracy})\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 300\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "        val_accuracy = correct_predictions.double() / len(val_dataset)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "    print(f'epoch: {epoch} train_loss {train_loss} val_loss {val_loss} val_accuracy {val_accuracy}')\n",
    "\n",
    "    # Implement Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        # Check early stopping condition\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print('Early stopping!')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([acc.cpu() for acc in val_accuracies], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\", inference_mode=False, r=16, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30335c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b411bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MiModelo.push_to_hub(repo_id=\"RikoteMaster/Bert_peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3191037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(mi_modelo, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d5924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
